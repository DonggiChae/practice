{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mask_r_cnn_practice.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSvszuYt7ehBm/XsIpmVHD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonggiChae/practice/blob/master/mask_r_cnn_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uJNq7Ff2--W"
      },
      "source": [
        "import torchvision\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import torch.nn as nn\r\n",
        "from torchvision.ops.boxes import nms, box_iou, clip_boxes_to_image, remove_small_boxes\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as T\r\n",
        "import torch.nn.init as nt\r\n",
        "from torchvision.ops import roi_align\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "# trf = T.compose([T.resize(),T.ToTensor()])\r\n",
        "# input_img = trf(img).unsqueeze(0)\r\n",
        "\r\n",
        "image = torch.zeros((1, 3, 800, 800)).float() \r\n",
        "\r\n",
        "bbox = torch.FloatTensor([[20, 30, 400, 500], [300, 400, 500, 600]]) # dummy box  x1, y1, x2, y2\r\n",
        "labels = torch.LongTensor([6, 8]) \r\n",
        "\r\n",
        "fe_size = 50\r\n",
        "\r\n",
        "\r\n",
        "dummy_img = torch.zeros((1, 3, 800,800)).float()\r\n",
        "# print(dummy_img)\r\n",
        "#Out: torch.Size([1, 3, 800, 800])\r\n",
        "\r\n",
        "\r\n",
        "model = torchvision.models.vgg16(pretrained=True)\r\n",
        "req_features = model.features[:30]\r\n",
        "faster_rcnn_fe_extractor = nn.Sequential(*req_features)\r\n",
        "out_map = faster_rcnn_fe_extractor(image)\r\n",
        "\r\n",
        "print(req_features)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb1L6Syf_-4u"
      },
      "source": [
        "# #Anchor Generator\r\n",
        "anchor_size = [128, 256, 512]\r\n",
        "anchor_ratio = [0.5, 1, 2]\r\n",
        "anchors = len(anchor_size) * len(anchor_ratio) * out_map.size(2) * out_map.size(3)\r\n",
        "anchor_boxes = torch.zeros((len(anchor_ratio) * len(anchor_size) * 50 * 50, 4) )\r\n",
        "\r\n",
        "ctr_x = np.arange(16, (fe_size+1) * 16, 16)\r\n",
        "ctr_y = np.arange(16, (fe_size+1) * 16, 16)\r\n",
        "\r\n",
        "ctr = np.zeros((2500, 2))\r\n",
        "\r\n",
        "def ctr_point(ctr_x, ctr_y):\r\n",
        "    index = 0\r\n",
        "    for x in range(len(ctr_x)):\r\n",
        "        for  y in range(len(ctr_y)):\r\n",
        "            ctr[index, 0] = ctr_x[x] - 8\r\n",
        "            ctr[index, 1] = ctr_y[y] - 8\r\n",
        "            index += 1\r\n",
        "    return ctr\r\n",
        "\r\n",
        "\r\n",
        "ctr_point(ctr_x, ctr_y)\r\n",
        "# print(ctr.shape)\r\n",
        "index = 0 \r\n",
        "for c_p in ctr:\r\n",
        "    ctr_x, ctr_y = c_p\r\n",
        "    for idx_s in range(len(anchor_size)):\r\n",
        "        for idx_r in range(len(anchor_ratio)):\r\n",
        "            h = anchor_size[idx_s] * np.sqrt(anchor_ratio[idx_r])\r\n",
        "            w = anchor_size[idx_s] * np.sqrt(1 / anchor_ratio[idx_r])\r\n",
        "            anchor_boxes[index, 0] =  ctr_x - w / 2\r\n",
        "            anchor_boxes[index, 1] =  ctr_y - h / 2\r\n",
        "            anchor_boxes[index, 2] =  ctr_x + w / 2\r\n",
        "            anchor_boxes[index, 3] =  ctr_y + h / 2\r\n",
        "            index += 1\r\n",
        "\r\n",
        "# print(anchor_boxes)\r\n",
        "\r\n",
        "\r\n",
        "valid_boxes = np.where((anchor_boxes[:, 0] >= 0) & \r\n",
        "                        (anchor_boxes[:, 1] >= 0) & \r\n",
        "                        (anchor_boxes[:, 2] <= 800) & \r\n",
        "                        (anchor_boxes[:, 3] <= 800))[0]\r\n",
        "\r\n",
        "vaild_anchors = anchor_boxes[valid_boxes]\r\n",
        "\r\n",
        "# print(vaild_anchors)\r\n",
        "# print(bbox)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0heu6OY_-0N"
      },
      "source": [
        "#iou\r\n",
        "pos_iou_threshold = 0.7\r\n",
        "neg_iou_threshold = 0.3\r\n",
        "\r\n",
        "\r\n",
        "ious = box_iou(vaild_anchors, bbox) * 10 #make something wrong result\r\n",
        "# print(ious)\r\n",
        "\r\n",
        "\r\n",
        "valid_labels = np.empty((valid_boxes.shape[0],))\r\n",
        "valid_labels.fill(-1)\r\n",
        "anchor_max_ious = ious.amax(axis=1)\r\n",
        "valid_labels[np.where(anchor_max_ious >= pos_iou_threshold)[0]] = 1\r\n",
        "valid_labels[np.where(anchor_max_ious < neg_iou_threshold)[0]] = 0\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foGcihbHW1Nw"
      },
      "source": [
        "#Determine vaild anchor, positve = 1, negative = 0 , (0.7 > i don't need (-1) > 0.3)\r\n",
        "pos_ratio = 0.5\r\n",
        "randomly_sample = 256\r\n",
        "\r\n",
        "\r\n",
        "total_pos = len(np.where(valid_labels == 1)[0])\r\n",
        "n_pos_sample = randomly_sample*pos_ratio if total_pos > randomly_sample*pos_ratio else total_pos\r\n",
        "n_neg_sample = randomly_sample - n_pos_sample\r\n",
        "n_pos_sample = int(n_pos_sample)\r\n",
        "n_neg_sample = int(n_neg_sample)\r\n",
        "\r\n",
        "pos_index = np.where(valid_labels == 1)[0]\r\n",
        "if len(pos_index) > randomly_sample*pos_ratio:\r\n",
        "    disable_index = np.random.choice(pos_index, size=len(pos_index)-n_pos_sample, replace=False)\r\n",
        "    valid_labels[disable_index] = -1\r\n",
        "\r\n",
        "neg_index = np.where(valid_labels == 0)[0]\r\n",
        "disable_index = np.random.choice(neg_index, size=len(neg_index) - n_neg_sample, replace=False)\r\n",
        "valid_labels[disable_index] = -1\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fy4azeeZU0J"
      },
      "source": [
        "#bbox regression \r\n",
        "# t_{x} = (x - x_{a})/w_{a}\r\n",
        "# t_{y} = (y - y_{a})/h_{a}\r\n",
        "# t_{w} = log(w/ w_a)\r\n",
        "# t_{h} = log(h/ h_a)\r\n",
        "\r\n",
        "\r\n",
        "argmax_ious = ious.argmax(axis=1)\r\n",
        "max_iou_bbox = bbox[argmax_ious]\r\n",
        "# print(max_iou_bbox.shape)\r\n",
        "\r\n",
        "height = vaild_anchors[:, 3] - vaild_anchors[:, 1]\r\n",
        "width = vaild_anchors[:, 2] - vaild_anchors[:, 0]\r\n",
        "ctr_y = vaild_anchors[:, 1] + 0.5 * height\r\n",
        "ctr_x = vaild_anchors[:, 0] + 0.5 * width\r\n",
        "\r\n",
        "base_height = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\r\n",
        "base_width = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\r\n",
        "base_ctr_y = max_iou_bbox[:, 1] + 0.5 * base_height\r\n",
        "base_ctr_x = max_iou_bbox[:, 0] + 0.5 * base_width\r\n",
        "\r\n",
        "\r\n",
        "# valid anchor boxes location\r\n",
        "eps = np.finfo(np.float64).eps\r\n",
        "height = np.maximum(height, eps) \r\n",
        "width = np.maximum(width, eps)\r\n",
        "dy = (base_ctr_y - ctr_y) / height\r\n",
        "dx = (base_ctr_x - ctr_x) / width\r\n",
        "dh = np.log(base_height / height)\r\n",
        "dw = np.log(base_width / width)\r\n",
        "anchor_locs = np.vstack((dx, dy, dw, dh)).transpose()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "anchor_labels = np.empty((len(anchor_boxes),), dtype=np.int32)\r\n",
        "anchor_locations = np.zeros((len(anchor_boxes), 4), dtype=np.float32)\r\n",
        "\r\n",
        "anchor_labels.fill(-1)\r\n",
        "anchor_labels[valid_boxes] = valid_labels\r\n",
        "print(anchor_labels.shape)\r\n",
        "\r\n",
        "anchor_locations[valid_boxes] = anchor_locs\r\n",
        "\r\n",
        "print(anchor_locations.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqpNtQccbOV2"
      },
      "source": [
        "#rpn\r\n",
        "#num_anchors = predicted box\r\n",
        "layer_weight = 0.1\r\n",
        "layer_bias = 0\r\n",
        "\r\n",
        "in_channels = 512\r\n",
        "num_anchors = len(anchor_size) * len(anchor_ratio)\r\n",
        "\r\n",
        "conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\r\n",
        "box_layer = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1, stride=1)\r\n",
        "score_layer = nn.Conv2d(in_channels, num_anchors * 2, kernel_size=1, stride=1)\r\n",
        "conv.weight.data.normal_(layer_weight, std=0.01)\r\n",
        "box_layer.weight.data.normal_(layer_weight, std=0.01)\r\n",
        "score_layer.weight.data.normal_(layer_weight, std=0.01)\r\n",
        "conv.bias.data.zero_()\r\n",
        "box_layer.bias.data.zero_()\r\n",
        "score_layer.bias.data.zero_()\r\n",
        "\r\n",
        "\r\n",
        "x = conv(out_map)\r\n",
        "pred_anchor_locs = box_layer(x)\r\n",
        "pred_cls_scores = score_layer(x)\r\n",
        "# print(pred_cls_scores)\r\n",
        "# print(pred_anchor_locs)\r\n",
        "\r\n",
        "pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\r\n",
        "pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous().view(1, -1, 2)\r\n",
        "objectness_score = pred_cls_scores.view(1, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# print(pred_anchor_locs.shape)\r\n",
        "# print(pred_cls_scores.shape)\r\n",
        "# print(objectness_score.shape)\r\n",
        "\r\n",
        "\r\n",
        "rpn_locs = pred_anchor_locs[0]\r\n",
        "rpn_scores = pred_cls_scores[0]\r\n",
        "\r\n",
        "\r\n",
        "# print(rpn_locs.shape)\r\n",
        "# print(rpn_scores.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3JjeSoIiJ4n"
      },
      "source": [
        "anchor_labels = torch.as_tensor(anchor_labels)\r\n",
        "# print(anchor_labels)\r\n",
        "rpn_cls_loss = F.cross_entropy(rpn_scores, anchor_labels.long(), ignore_index=-1)\r\n",
        "# print(rpn_cls_loss)\r\n",
        "\r\n",
        "sampled_pos_inds_subset = anchor_labels > 0\r\n",
        "# print(sampled_pos_inds_subset)\r\n",
        "# print(anchor_locations)\r\n",
        "regression_targets = anchor_locations[sampled_pos_inds_subset]\r\n",
        "pred_target = rpn_locs[sampled_pos_inds_subset]\r\n",
        "\r\n",
        "# print(regression_targets)\r\n",
        "# print(pred_target)\r\n",
        "pred_target = pred_target\r\n",
        "regression_targets = torch.from_numpy(regression_targets)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "x = torch.abs(regression_targets - pred_target)\r\n",
        "rpn_loc_loss = ((x<0.5).float()*(x**2)*0.5 + (x>0.5).float()*(x-0.5)).sum()\r\n",
        "\r\n",
        "# print(rpn_loc_loss)\r\n",
        "\r\n",
        "rpn_lambda = 10.\r\n",
        "N_reg = sampled_pos_inds_subset.float().sum() # number of bounding box for positve anchors\r\n",
        "rpn_loc_loss = rpn_loc_loss / N_reg\r\n",
        "rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss) # something wrong???\r\n",
        "# print(N_reg)\r\n",
        "# print(rpn_loc_loss)\r\n",
        "# print(rpn_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iurnLuFzY0Y"
      },
      "source": [
        "#Non-maximum supression(NMS)\r\n",
        "\r\n",
        "nms_thresh = 0.7\r\n",
        "n_train_pre_nms = 12000\r\n",
        "n_train_post_nms = 2000\r\n",
        "n_test_pre_nms = 6000\r\n",
        "n_test_post_nms = 300\r\n",
        "min_size = 16\r\n",
        "\r\n",
        "anc_height = anchor_boxes[:, 3] - anchor_boxes[:, 1]\r\n",
        "anc_width = anchor_boxes[:, 2] - anchor_boxes[:, 0]\r\n",
        "anc_ctr_y = anchor_boxes[:, 1] + 0.5 * anc_height\r\n",
        "anc_ctr_x = anchor_boxes[:, 0] + 0.5 * anc_width\r\n",
        "print(anc_width)\r\n",
        "\r\n",
        "# The 22500 anchor boxes location and labels predicted by RPN\r\n",
        "pred_anchor_locs_numpy = pred_anchor_locs[0].data.numpy()\r\n",
        "objectness_score_numpy = objectness_score[0].data.numpy()\r\n",
        "dx = pred_anchor_locs_numpy[:, 0::4] \r\n",
        "dy = pred_anchor_locs_numpy[:, 1::4] \r\n",
        "dw = pred_anchor_locs_numpy[:, 2::4] \r\n",
        "dh = pred_anchor_locs_numpy[:, 3::4] \r\n",
        "\r\n",
        "# print(dx)\r\n",
        "# print(dy)\r\n",
        "\r\n",
        "\r\n",
        "roi = np.zeros(pred_anchor_locs_numpy.shape)\r\n",
        "roi[:, 0::4] = dx - 0.5 * dw\r\n",
        "roi[:, 1::4] = dy - 0.5 * dh\r\n",
        "roi[:, 2::4] = dx + 0.5 * dw\r\n",
        "roi[:, 3::4] = dy + 0.5 * dh\r\n",
        "\r\n",
        "roi = torch.from_numpy(roi)\r\n",
        "print(roi)\r\n",
        "\r\n",
        "boxes = clip_boxes_to_image(roi, [800,800])\r\n",
        "boxes = remove_small_boxes(boxes, 16)\r\n",
        "score = objectness_score_numpy[boxes]\r\n",
        "order = score.ravel().argsort()[::-1]\r\n",
        "print(order)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvhjeVZTeUye"
      },
      "source": [
        "#Take nms topN \r\n",
        "order = order[:n_train_pre_nms]\r\n",
        "order = np.sort(order)\r\n",
        "order = torch.from_numpy(order)\r\n",
        "# print(order)\r\n",
        "# print(roi.shape)\r\n",
        "rois = roi[order]\r\n",
        "print(score.shape)\r\n",
        "scores = torch.from_numpy(score[order]).type(torch.DoubleTensor)\r\n",
        "print(rois.shape)\r\n",
        "print(scores)\r\n",
        "print(scores.shape)\r\n",
        "keep = nms(rois, scores, nms_thresh)\r\n",
        "print(keep.shape)\r\n",
        "post_train_valid_rois = rois[keep][:n_train_post_nms] #??????????????\r\n",
        "post_train_valid_scores = scores[keep][:n_train_post_nms] #????????\r\n",
        "print(post_train_valid_rois.shape) \r\n",
        "print(post_train_valid_scores.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF0y5gpiq9QZ"
      },
      "source": [
        "# Proposal targets\r\n",
        "\r\n",
        "n_sample = 128\r\n",
        "pos_ratio = 0.25\r\n",
        "pos_iou_thresh = 0.5\r\n",
        "neg_iou_thresh_hi = 0.5\r\n",
        "neg_iou_thresh_lo = 0.0\r\n",
        "\r\n",
        "\r\n",
        "ious = box_iou(post_train_valid_rois, bbox)\r\n",
        "print(ious.shape) \r\n",
        "\r\n",
        "\r\n",
        "bbox_assignments = ious.argmax(axis=1)\r\n",
        "roi_max_ious = ious.max(axis=1)\r\n",
        "roi_target_labels = labels[bbox_assignments]\r\n",
        "print(roi_target_labels.shape)\r\n",
        "print(roi_max_ious)\r\n",
        "\r\n",
        "\r\n",
        "total_n_pos = len(np.where(roi_max_ious[0] >= pos_iou_thresh)[0])\r\n",
        "n_pos_sample = n_sample*pos_ratio if total_n_pos > n_sample*pos_ratio else total_n_pos\r\n",
        "n_neg_sample = n_sample - n_pos_sample\r\n",
        "\r\n",
        "print(n_pos_sample) \r\n",
        "print(n_neg_sample) \r\n",
        "\r\n",
        "pos_index = np.where(roi_max_ious[0] >= pos_iou_thresh)[0]\r\n",
        "pos_index = np.random.choice(pos_index, size=n_pos_sample, replace=False)\r\n",
        "\r\n",
        "neg_index = np.where((roi_max_ious[0] < neg_iou_thresh_hi) & (roi_max_ious[0] > neg_iou_thresh_lo))[0]\r\n",
        "neg_index = np.random.choice(neg_index, size=n_neg_sample)\r\n",
        "\r\n",
        "keep_index = np.append(pos_index, neg_index)\r\n",
        "post_sample_target_labels = roi_target_labels[keep_index].data.numpy()\r\n",
        "post_sample_target_labels[len(pos_index):] = 0\r\n",
        "post_sample_rois = post_train_valid_rois[keep_index]\r\n",
        "print(post_sample_rois.shape)\r\n",
        "\r\n",
        "post_sample_bbox = bbox[bbox_assignments[keep_index]]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "height = post_sample_rois[:, 3] - post_sample_rois[:, 1]\r\n",
        "width = post_sample_rois[:, 2] - post_sample_rois[:, 0]\r\n",
        "ctr_y = post_sample_rois[:, 1] + 0.5 * height\r\n",
        "ctr_x = post_sample_rois[:, 0] + 0.5 * width\r\n",
        "# print(height)\r\n",
        "\r\n",
        "base_height = post_sample_bbox[:, 3] - post_sample_bbox[:, 1]\r\n",
        "base_width = post_sample_bbox[:, 2] - post_sample_bbox[:, 0]\r\n",
        "base_ctr_y = post_sample_bbox[:, 1] + 0.5 * base_height\r\n",
        "base_ctr_x = post_sample_bbox[:, 0] + 0.5 * base_width\r\n",
        "\r\n",
        "\r\n",
        "eps = np.finfo(np.float64).eps\r\n",
        "height = np.maximum(height, eps)\r\n",
        "width = np.maximum(width, eps)\r\n",
        "\r\n",
        "\r\n",
        "dy = (base_ctr_y - ctr_y) / height\r\n",
        "dx = (base_ctr_x - ctr_x) / width\r\n",
        "dh = np.log(base_height / height)\r\n",
        "dw = np.log(base_width / width)\r\n",
        "\r\n",
        "\r\n",
        "gt_roi_locs = np.vstack((dy, dx, dh, dw)).transpose()\r\n",
        "# print(gt_roi_locs.size)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HTIZmLX-Wc1"
      },
      "source": [
        "#Fast R-CNN\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "roi_indexes = 0 * np.ones((len(post_sample_rois),))\r\n",
        "roi_indexes = torch.from_numpy(roi_indexes).float()\r\n",
        "print(rois.shape, roi_indexes.shape)\r\n",
        "indexes_and_rois = torch.cat([roi_indexes[:, None], post_sample_rois], dim=1)\r\n",
        "# indexes_and_rois = indexes_and_rois.DoubleTensor()\r\n",
        "out_map = torch.tensor(out_map, dtype=torch.double)\r\n",
        "# print(indexes_and_rois)\r\n",
        "# print(out_map)\r\n",
        "roi_aligns = roi_align(out_map, indexes_and_rois , output_size = 7, spatial_scale = 1, sampling_ratio = -1, aligned = False)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3yqmc6CkYJZ"
      },
      "source": [
        "print(roi_aligns.shape)\r\n",
        "output_roi_aligns = roi_aligns.view(roi_aligns.size(0), -1)\r\n",
        "print(output_roi_aligns.shape)\r\n",
        "output_roi_aligns = torch.tensor(output_roi_aligns, dtype=torch.float)\r\n",
        "\r\n",
        "roi_head = nn.Sequential(nn.Linear(25088, 4096),\r\n",
        "                        nn.Linear(4096, 4096))\r\n",
        "\r\n",
        "cls_loc = nn.Linear(4096, 21*4)\r\n",
        "cls_loc.weight.data.normal_(0, 0.01)\r\n",
        "cls_loc.bias.data.zero_()\r\n",
        "\r\n",
        "cls_score = nn.Linear(4096, 21)\r\n",
        "cls_score.weight.data.normal_(0, 0.01)\r\n",
        "cls_score.bias.data.zero_()\r\n",
        "\r\n",
        "x = roi_head(output_roi_aligns)\r\n",
        "roi_cls_loc = cls_loc(x)\r\n",
        "roi_cls_score = cls_score(x)\r\n",
        "\r\n",
        "print(roi_cls_loc.shape, roi_cls_score.shape)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL4BFVbQsI2R"
      },
      "source": [
        "print(gt_roi_locs.shape)\r\n",
        "gt_roi_cls_loc = gt_roi_locs\r\n",
        "gt_roi_cls_label = torch.from_numpy(post_sample_target_labels).long()\r\n",
        "print(roi_cls_score.shape)\r\n",
        "\r\n",
        "roi_cls_loss = F.cross_entropy(roi_cls_loc, gt_roi_cls_label, ignore_index=-1)\r\n",
        "\r\n",
        "num_roi = roi_cls_loc.size(0)\r\n",
        "roi_cls_loc = roi_cls_loc.view(-1, 21, 4)\r\n",
        "roi_cls_loc = roi_cls_loc[torch.arange(num_roi), gt_roi_cls_label]\r\n",
        "print(roi_cls_loc.shape)\r\n",
        "\r\n",
        "mask = gt_roi_cls_label>0\r\n",
        "mask_loc_pred = roi_cls_loc[mask]\r\n",
        "mask_loc_target = gt_roi_cls_loc[mask]\r\n",
        "\r\n",
        "print(mask_loc_pred.shape)#????????????\r\n",
        "print(mask_loc_target.shape)#???????????????\r\n",
        "\r\n",
        "x = torch.abs(mask_loc_pred-mask_loc_target)\r\n",
        "roi_loc_loss = ((x<0.5).float()*x**2*0.5 + (x>0.5).float()*(x-0.5)).sum()\r\n",
        "print(roi_loc_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vbq57ZgGBEw"
      },
      "source": [
        "mask_fcn = nn.Sequential(\r\n",
        " nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\r\n",
        " nn.ReLU(inplace=True),\r\n",
        " nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\r\n",
        " nn.ReLU(inplace=True),\r\n",
        " nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\r\n",
        " nn.ReLU(inplace=True),\r\n",
        " nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\r\n",
        " nn.ReLU(inplace=True),\r\n",
        " nn.ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2)),\r\n",
        " nn.ReLU(inplace=True),\r\n",
        " nn.Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\r\n",
        ")\r\n",
        "\r\n",
        "mask_head = nn.Sequential(*mask_fcn)\r\n",
        "masks = mask_head(output_roi_aligns)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1G4J7ypAjc7"
      },
      "source": [
        "#roi_align\r\n",
        "\r\n",
        "\r\n",
        "roi = roi_align(input, boxes, output_size = 7, spatial_scale)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Loss function\r\n",
        "\r\n",
        "reg_loss = 0\r\n",
        "\r\n",
        "def smooth_l1_loss(predicted_bbox, bbox_target):\r\n",
        "    n= torch.abs(predicted_bbox - bbox_target)\r\n",
        "    if n < 1:\r\n",
        "        loss = 0.5 * n ** 2\r\n",
        "    else:\r\n",
        "        loss = n - 0.5\r\n",
        "    return loss\r\n",
        "\r\n",
        "\r\n",
        "classification_loss = F.cross_entropy(class_score, labels)\r\n",
        "\r\n",
        "\r\n",
        "rpn_cls_loss = classification_loss\r\n",
        "rpn_loc_loss = smooth_l1_loss(predicted_bbox, bbox_target).sum()\r\n",
        "\r\n",
        "#rpn loss function\r\n",
        "rpn_lambda = 10.\r\n",
        "N_reg = (gt_rpn_score >0).float().sum() # number of bounding box for positve anchors\r\n",
        "rpn_loc_loss = rpn_loc_loss / N_reg\r\n",
        "rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss)\r\n",
        "\r\n",
        " \r\n",
        "#  x = #pixel \r\n",
        "# mask_binary_loss = sigmoid()\r\n",
        "\r\n",
        "\r\n",
        "def maskrcnn_inference(x, labels):\r\n",
        "    # type: (Tensor, List[Tensor]) -> List[Tensor]\r\n",
        "    \"\"\"\r\n",
        "    From the results of the CNN, post process the masks\r\n",
        "    by taking the mask corresponding to the class with max\r\n",
        "    probability (which are of fixed size and directly output\r\n",
        "    by the CNN) and return the masks in the mask field of the BoxList.\r\n",
        "    Args:\r\n",
        "        x (Tensor): the mask logits\r\n",
        "        labels (list[BoxList]): bounding boxes that are used as\r\n",
        "            reference, one for ech image\r\n",
        "    Returns:\r\n",
        "        results (list[BoxList]): one BoxList for each image, containing\r\n",
        "            the extra field mask\r\n",
        "    \"\"\"\r\n",
        "    mask_prob = x.sigmoid()\r\n",
        "\r\n",
        "    # select masks corresponding to the predicted classes\r\n",
        "    num_masks = x.shape[0]\r\n",
        "    boxes_per_image = [label.shape[0] for label in labels]\r\n",
        "    labels = torch.cat(labels)\r\n",
        "    index = torch.arange(num_masks, device=labels.device)\r\n",
        "    mask_prob = mask_prob[index, labels][:, None]\r\n",
        "    mask_prob = mask_prob.split(boxes_per_image, dim=0)\r\n",
        "\r\n",
        "    return mask_prob\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# (mask_head): MaskRCNNConvUpsampleHead(\r\n",
        "#       (mask_fcn1): Conv2d(\r\n",
        "#         256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\r\n",
        "#         (activation): ReLU()\r\n",
        "#       )\r\n",
        "#       (mask_fcn2): Conv2d(\r\n",
        "#         256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\r\n",
        "#         (activation): ReLU()\r\n",
        "#       )\r\n",
        "#       (mask_fcn3): Conv2d(\r\n",
        "#         256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\r\n",
        "#         (activation): ReLU()\r\n",
        "#       )\r\n",
        "#       (mask_fcn4): Conv2d(\r\n",
        "#         256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\r\n",
        "#         (activation): ReLU()\r\n",
        "#       )\r\n",
        "#       (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\r\n",
        "#       (deconv_relu): ReLU()\r\n",
        "#       (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\r\n",
        "\r\n",
        "\r\n",
        "# L = L_cls + L_box + L_mask\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}